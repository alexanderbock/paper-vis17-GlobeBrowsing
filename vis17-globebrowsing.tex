\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused. Also, the teaser figure should only have the
%% width of the abstract as the template enforces it.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage[utf8]{inputenc}
\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
%\usepackage{color}
\usepackage{xcolor} % Some more colors not defined in "color" package
% \usepackage{tabu}                      % only used for the table example
% \usepackage{booktabs}                  % only used for the table example
\usepackage{todonotes}
\usepackage[draft]{hyperref} % Weird error occurs without this.
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[]{algorithm2e}
\usepackage{textgreek}
\usepackage{tikz, pgfplots}
\usepackage{gensymb}

\newcommand{\kallecomment}[1]{\textbf{[-Kalle-~}
    \textcolor{orange}{#1}
    \textbf{~]}}

\newcommand{\emilcomment}[1]{\textbf{[-Emil-~}
    \textcolor{red}{#1}
    \textbf{~]}}

\newcommand{\alexcomment}[1]{\textbf{[-Alex-~}
    \textcolor{magenta}{#1}
    \textbf{~]}}

\newcommand{\anderscomment}[1]{\textbf{[-Anders-~}
    \textcolor{cyan}{#1}
    \textbf{~]}}
\newcommand{\plgrem}[1]{\textcolor{blue}{~\textbf{!!}~}}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}

\newcommand{\etal}{\emph{et~al.}}
\newcommand{\dsup}[1]{\textsuperscript{#1}}

\setlength{\fboxsep}{0pt}

\newcommand{\denselist}{\itemsep 0pt\parsep=1pt\partopsep 0pt}


%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in IEEE Transactions on Visualization and Computer Graphics.}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{120}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}
%% please declare the paper type of your paper to help reviewers, only shown in review mode
%% choices:
%% * algorithm/technique
%% * application/design study
%% * evaluation
%% * system
%% * theory/model
\vgtcpapertype{application}

%% Paper title.

\title{Globe Browsing: Contextualized Spatio-Temporal \\
Planetary Surface Visualization}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Karl Bladin, Emil Axelsson, Erik Broberg, Carter Emmart, Patric Ljung, \\ Alexander Bock, \textit{Member, IEEE}, and Anders Ynnerman, \textit{Associate Member, IEEE}}

\authorfooter{
%% insert punctuation at end of each item
\item
Karl Bladin, Emil Axelsson, Patric Ljung, Anders Ynnerman and Erik Broberg are with Link\"oping University.
E-mail: \{ karl.bladin, emil.axelsson, patric.ljung, anders.ynnerman \} @liu.se, eribr049@student.liu.se
\item
Carter Emmart is with American Museum of Natural History. E-mail: carter@amnh.org.
\item
Alexander Bock is with New York University and Link\"oping University. E-mail: alexander.bock@nyu.edu
}

%other entries to be set up for journal
\shortauthortitle{Bladin \MakeLowercase{\textit{et al.}}: Globe Browsing}

%% Abstract section.
\abstract{%
% Submitted version of the abstract
%We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial data of celestial bodies for use in public science dissemination and communication between domain scientists.
%Results of planetary mapping is often shared openly for use in scientific research and mission planning.
%However, this data is, in its raw format, not accessible to non-experts, as it is difficult to grasp the context of the data and the intricate acquisition process.
%As our approach is full interactive and handles dynamic data sources, we are significantly shortening the time between discovery and dissemination of data and results.
%We describe the image acquisition, required pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps as well as high-resolution digital terrain models.
%One interesting case is Mars as extensive amount of map data is available from various missions.
%A second case is dynamic processes, such as weather conditions on Earth that require spatio-temporal datasets.
%In addition to visualizing the scientific results, we also present ongoing data acquisition, exemplified in the Pluto flyby of the New Horizons spacecraft.
%Our work has been implemented in the OpenSpace software, which enables interactive presentations in a range of environments such as immersive dome theaters and virtual reality headsets.
%
Results of planetary mapping are often shared openly for use in scientific research and mission planning.
In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process.
We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication.
As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results.
We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models.
The results are demonstrated for three different celestial bodies.
The first case addresses high-resolution map data on the surface of Mars.
A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets.
As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto.
We visualize the acquisition process as well as the resulting surface data.
Our work has been implemented in the OpenSpace software~\cite{Bock2017}, which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.

%
% In addition, the application supports temporal datasets, where available, in order to visualize a celestial body's dynamic processes, such as weather conditions on Earth.
% Furthermore, we interactively play back in-situ visualizations of the process for data acquisition.
%
%
%We present an application where scientific mapping data of celestial bodies is contextualized in space and time to enrich the experience in public dissemination as well as to support communication between scientists.
%In our system various types of datasets and visualization techniques are combined.
%A chunked level-of-detail approach is used to enable interactive exploration of global maps as well as local high-resolution digital terrain models with textures and height information.
%This is particularly interesting for Mars thanks to the extensive amount of map data gathered by the various missions to this planet.
%Using time varying datasets, we visualize the dynamics of a celestial body, such as weather conditions on Earth.
%Furthermore, we interactively play back in-situ visualizations of the process for data acquisition.
%
%Our work has been implemented in the open source software OpenSpace, which enables interactive presentations in immersive environments like dome theaters and virtual reality headsets.
%
%
%
%
%
%The mapping of the planets and moons within our solar system is carried out by space organizations such as NASA and ESA.
%Image data are collected by cameras on satellites and spacecraft, and used in scientific research and in planning of future space missions.
%Despite the fact that NASA shares a lot of its image data openly, it seldom reaches the general public.
%
%A lot of research has been done in terms of visualizing planetary data using terrain rendering. Current technology relies on techniques such as out-of-core, and dynamic level-of-detail rendering, multi threaded data acquisition and consideration of precision limitations.
%
%These systems are often either specialized for researchers where accuracy is the main concern, or for games, where the user experience plays a major role.
%
%We present an application for visualizing the same data that space scientists analyze in the context of a virtual environment representing the space that we are exploring.
%Given the contextualization that the real space provides, we can model the solar system using positional data for modelling the planets' orbits together with the space probes that explores them.
%In a time varying visualizations we can show the dynamics of a planetary surface as well as the data collection process itself.
%
%The main focus of this paper is to describe the combination of different datasets gathered for mapping out planets' surface features. 
% particularly Mars due to the extensive amount of map data gathered by the various missions on this planet.
%
%Our rendering system combines various resolutions of local and global map datasets and presents them together in their real context.
%Using the Geospatial Data Abstraction Library (GDAL), we can preprocess images to match the most common cylindrical projection format used for globe rendering.
%A chunked level-of-detail approach is used for rendering tiles which are height mapped on the fly.
%This allows for a versatile globe renderer which can easily load new datasets without requiring preprocessing by the rendering software.
%
%We can use the same data that scientists use in their research for public dissemination and present it, bundled and contextualized, using immersive rendering systems.
%Our software, OpenSpace, is an open source project with the goal of bringing space science to the general public.
%
%
% The mapping of the planets and moons within our solar system is carried out by space organizations such as NASA and
% ESA. Image data are collected by instruments on satellites and spacecraft, and subsequently used in scientific research and mission
% planning. While map data from space missions are often shared openly, it can be difficult for non-experts to comprehend the context of
% the data and understand the acquisition process.
% We present an application where scientific mapping data of celestial bodies is contextualized in space and time to enrich the experience
% in public dissemination as well as to support communication between scientists. In our system various types of datasets and
% visualization techniques are combined; A chunked level-of-detail approach is used to enable interactive exploration of global maps as
% well as local high-resolution digital terrain models with textures and height information. This is particularly interesting for Mars thanks
% to the extensive amount of map data gathered by the various missions to this planet. Using time varying datasets, we visualize the
% dynamics of a celestial body, such as weather conditions on Earth. Furthermore, we interactively play back in-situ visualizations of the
% process for data acquisition.
% Our work has been implemented in the open source software OpenSpace that enables interactive presentations in immersive
% environments like dome theaters and virtual reality headsets.
%
%
} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Astronomical visualization, globe rendering, public dissemination, science communication, space mission visualization }

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{ % not used in journal version
 \CCScat{K.6.1}{Management of Computing and Information Systems}%
{Project and People Management}{Life Cycle};
 \CCScat{K.7.m}{The Computing Profession}{Miscellaneous}{Ethics}
}

%% Uncomment below to include a teaser figure.
\teaser{
  \centering
  \fbox{\includegraphics[width=\linewidth, height=0.35\linewidth]{figures/teaser2.jpg}}
  %\includegraphics[width=\linewidth]{CypressView}
  %\caption{In the Clouds: Vancouver from Cypress Mountain. Note that the teaser may not be wider than the abstract block.}
%	\label{fig:teaser}
}

%% Uncomment below to disable the manuscript note
%\renewcommand{\manuscriptnotetxt}{}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%\vgtcinsertpkg

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction} \label{sec:introduction}
\maketitle

%\emilcomment{TODO: Need larger labels, cannot read}
%\emilcomment{TODO: Video}
%\alexcomment{TODO: Check Section 4.4}
%\kallecomment{TODO: Add information about Chunk rendering to 4.4}
%\kallecomment{TODO: Add related work about level of detail}
%\alexcomment{TODO: Check 4.5 Cluster Rendering}
%\alexcomment{TODO: Write Results section}
%\kallecomment{TODO: Fix up reference}
%\kallecomment{TODO: Figure 8 caption}
%\alexcomment{TODO: Figure 9 caption}
%\alexcomment{TODO: The submission ID is missing} % Not needed I think, not a double blind review

Throughout history, the study of celestial bodies orbiting our Sun has always intrigued mankind and played a central role in mythology and culture, and contributed to the scientific discovery of the fundamental laws of physics.
Since the invention of the telescope in the 17\dsup{th} century, our knowledge of the planets and other celestial bodies in our solar system has grown at an ever-increasing rate.
The quest for more knowledge made a leap forward in 1962 when \emph{National Aeronautics and Space Administration's} (NASA's) Mariner 2 flew by Venus and paved the path for a long-term, and still ongoing, systematic mapping of the solar system by spacecraft flying by or orbiting objects of interest.
Another landmark in the exploration of the solar system was the Viking program, launched in the year 1975, which gathered important information about Mars and its surface features from the two orbiting satellites and the landers put on the surface of the planet.

New Horizons' Pluto flyby in 2015 is yet another milestone as Pluto was the last planet\footnote{When New Horizons was launched in 2006, Pluto was still defined as a planet. It has since been reclassified as a dwarf planet.} to be visited by a space probe. As a consequence of the spacecraft-based exploration of the solar system we now have detailed surface data from a range of celestial bodies orbiting the sun, including some located in the most remote places of the solar system. 

\begin{figure*}
\includegraphics[width=\linewidth]{figures/overview.pdf}
\caption{Illustration of the processing pipeline of the presented application. Raw images acquired from spacecraft can be used as-is, by utilizing orientation information, and projecting images directly onto celestial bodies, or, they can be processed and made available as online global map resources. Our system supports both approaches, as in the cases with the Pluto flyby of New Horizons and the several missions to Mars. This puts high demands on the system to interactively render high-resolution images as well as reducing the time from image acquisition to dissemination.}\vspace{-3mm}

\label{fig:procpipe}
\end{figure*}

This large collection of data has enabled numerous scientific discoveries about the geological structure of the planets and many of these have been based on the use of image processing and visualization.
The work presented in this paper focuses on providing visualization tools and applications for communication of these discoveries, and the engineering efforts which enabled them.
The developed application is an example of a new data-driven and interactive paradigm of science communication, which provides a means of bridging the gap between experts and the general public, but also serves as a useful tool for communication between teams of experts working on complex missions and scientific exploration.
We are focusing on the use of open surface data made available by the different space agencies and research laboratories from which high-resolution and scientifically accurate representations of celestial bodies are created using geometry extraction and texturing, enabling exploration in \emph{Globe Browsing} sessions.
%Visualization then provides a unique possibility to contextualize scientific exploration of our solar system, as well as the challenging engineering efforts behind space exploration, and, indeed, creates the ability to virtually visit the surface of celestial bodies. 

There are several technical challenges involved in accessing and preparing the data for interactive globe rendering which stem from aspects of collected data such as size, complexity, numerical precision and accuracy, need for curvature corrections, and incompleteness and variety of sources.
Furthermore, the use of multiresolution visualization approaches to out-of-core data streaming from remote repositories and navigation models spanning from surfaces to interplanetary space requires dynamic data flow and navigation control. 

We have chosen to address the challenges in the context of three representative application scenarios for different celestial bodies.
As a first example we show how our system can enable interactive and seamless surface exploration of Mars with previously unprecedented resolution.
We then show, as an example of temporal data, how daily images of the Earth, can be accessed and visualized from vantage points in interplanetary space as well as in close ups of regions of interest in order to communicate dynamic processes such as atmospheric events.
Finally, we present how complex data gathering can be visualized by virtually following the New Horizons flyby of Pluto in 2015, and tapping into the data flows from the instruments on the probe, showing how high-resolution image mosaics are constructed, thus, dramatically reducing the time between discovery and dissemination.

At the core of our approach lies the gathering of surface image data and projecting it to a common projection format either as a preprocessing step or as an integrated part of the rendering process to visualize the data acquisition.
The datasets are rendered as layers, projected on the surface of globes.
\fig{procpipe} presents an overview of this processing pipeline.
In our work we followed four underpinning principles that guided the design and implementation;
\vspace*{-1.25mm}
\begin{itemize}\denselist
  \item Visualization should be data-driven, and thus only data derived from the instruments on space probes should be used,
  \item Data from multiple instruments, and/or probes, should be merged to represent the aggregated knowledge of an object,
  \item The maximum available resolution in space and time should be accessible on demand for the areas of interest,
  \item Data should be as current as possible by enabling access to a range of curated and frequently updated repositories.
\end{itemize}

\vspace*{-1.25mm}

We provide a reference implementation of this system in the open source astrovisualization framework \emph{OpenSpace}, which targets data exploration and science communication on a range of platforms, including large scale dome theaters, virtual reality headsets, and touch tables, and thus, enabling spatio-temporal navigation and contextualization of satellites and space probes together with celestial bodies.

Our contributions can be summarized as:
\vspace*{-1.25mm}
\begin{itemize}\denselist
\item A visualization pipeline and platform for contextualized multiresolution spatio-temporal data of celestial bodies,
%\item Tailoring of data handling, visualization, and interaction methods integrated into the pipeline,
\item A clear focus on the ability to accurately visualize the acquisition of this data using the pipeline to show the accumulation of knowledge about celestial bodies.
\item Three representative application scenarios demonstrating the utility and flexibility of the system.
\end{itemize}

\vspace*{-2.25mm}

\begin{figure*}
\centering
    \begin{subfigure}[t]{0.33\linewidth}
    	\fbox{\includegraphics[width=\linewidth, height=0.6\linewidth]{figures/mars_ganges.png}}
      	\caption{High resolution terrain and texture on Mars}
    	\label{fig:marsganges}
    \end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
    	\fbox{\includegraphics[width=\linewidth, height=0.6\linewidth]{figures/earth_atm.png}}
      	\caption{Time varying datasets on Earth}
      	\label{fig:earthdynamic}
    \end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
    	\fbox{\includegraphics[width=\linewidth, height=0.6\linewidth]{figures/pluto_nh.png}}
      \caption{Image acquisition on Pluto}
    \end{subfigure}
    \caption{Our system demonstrated with the three described scenarios. a) Light toned mounds in Ganges Chasma on Mars using a local HiRISE patch with 25\,cm resolution and MOLA elevation with a global mosaic of CTX images and a global color basemap. b) The top row shows Earth reflectance animated over three days in June 2017. The color layer is a corrected reflectance global map taken by the VIIRS satellite. The bottom row shows the difference in maximum air temperature between June 1985 and 2015. Both layers have daily coverage provided by NASA GIBS. c) New Horizons image campaign of the Pluto system. The image shows Pluto and its moon Charon with the view frustum of New Horizons' LORRI instrument. These scenarios are used in public presentations to explain their respective scientific discoveries to public audiences.}\vspace{-3mm}
    \label{fig:scenarios}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Related Work %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work} \label{sec:relatedwork}

%\begin{enumerate}
%\item Astronomical visualization in general.\plgrem{I will try to compile a general collection of astro vis citations}
%\item Astrovis for communication - planetariums
%2. Go towards Science communication and contextualization.
%\item Related GIS work
%\item LOD - look up the real references from the LOD book. It is not OK to refer to pages in the book.
%
%\end{enumerate}

The presented application builds on technologies and methods spreading over four major topics, namely science communication, large-scale astronomical visualization, geographical information systems (GIS), and level-of-detail (LOD) rendering methods for (planetary scale) terrains. Our review of related work thus follows the same structure where we outline previous methods and approaches and conclude with commentaries on how it relates to the presented application.

%Our application builds on previous work in the fields of large-scale astronomical visualization, geographical information systems (GIS), and level-of-detail (LOD) rendering.

\noindent\textbf{Science communication} ---
Visual communication and the use of animations and interactive techniques through computer graphics are at the core of communicating scientific findings and contextualizing information. It enables a wide appeal across ages and cultures.
Virtual reality environments such as planetariums~\cite{magnor2010progress, liu20013} have always been in the focus of educators, but a shift towards supporting data inspection on consumer grade hardware is ongoing~\cite{nakasone2009astrosim}.
Notable examples of applications in which large scale-differences are visualized include movie productions, a classic example is \emph{Powers of Ten} by Eames~\cite{powersOfTen} from 1977, but also include current interactive software packages such as Uniview~\cite{KHECY10}, DigiStar~\cite{ES16}, and DigitalSky~\cite{Sky16}, which are all based on curated datasets containing information about our Universe~\cite{abbott2006digital}. 
  %Another more recent effort by Weiskopf~\etal~\cite{weiskopf2006explanatory} aims at an explanation of the special and general relativity.

Recent advances in graphics hardware and processing power in consumer hardware has enabled a broad use of actual scientific data in applications aimed for science communication for the public at science centers and elsewhere.
One such story is presented by Ynnerman~\etal~\cite{ynnerman2016interactive}.
The ability of providing the same data as scientists have used in their explorations to the layman user is currently a major change and a paradigm shift in science communication.

\noindent\textbf{Astronomical visualization} ---
One of the challenges in astrovizualization is dealing with positioning and navigation over extreme distances and there are several publications describing approaches to this problem, such as the notion of the ScaleGraph, introduced by Klashed \etal~\cite{KHECY10}.
Hanson~\etal~\cite{hanson2000very} introduced the concept of \emph{power scaled coordinates} (PSC) to handle the large numbers for astronomical distances, and subsequently Fu~\etal~\cite{fu2006navigation} categorized the problem of large scale navigation and positioning. Fu and Hanson also introduced a depth buffer remapping to cover a wider range of distances than possible with a fixed point depth buffer and conventional near and far planes~\cite{fu2007transparently}.
%Other efforts include the work by Li~\etal~\cite{li2006scalable} who introduce a world-in-miniature techniques based on logarithmic landmarks for navigation support using PSC.
In our presented work we use the Dynamic Scene Graph, proposed by Axelsson \etal~\cite{Axelsson2017Dynamic}, as it allows for seamless navigation across large distances without loss of precision.
%There are several other contributions to the visualization field with astrophysics as its motivation such as the work on visualization on relativity by Weiskopf~\etal~\cite{weiskopf2005visualization} and the multiwavelength data visualization by Li~\etal~\cite{li2008visualizing}. An examples of how visualization has played a documented role in the scientific exploration is the work on interactive visualization of galaxy surveys by Punzo~\etal~\cite{punzo2015role}.

%%% End of paste from Alex, start of Alex's previous notes.
Relevant to the work presented here are a number of freely available astronomical software packages, such as, Eyes on the Solar System by NASA~\cite{hussey2010eyes}, also tools for planetary rendering in free software such as Blender~\cite{kent2013visualizing, naiman2016astroblend} can provide engaging experiences for a wide range of users. These solutions are focusing on general astronomical visualization and do not provide the out-of-core level-of-detail surface rendering required to visualize globes in high detail.
Another related astrovisualization package is the \emph{World Wide Telescope} (WWT) which brings together many data sources into a unified environment \cite{ali2011online, goodman2012worldwide}.
Among these are static planetary datasets rendered using reprojection to the \emph{Tessellated Octahedral Adaptive Subdivision Transform} (TOAST) format which poses issues on accuracy and in the availability of map datasets due to flat tiles and reprojection requirements \cite{berriman2017application}. The projection is therefore more suitable for sky-tessellation as it does not require an ellipsoidal model and the dataset can be maintained in the same projection format.

There are several ongoing efforts providing access to planetary data through interactive globe visualization softwares. Cesium by Cozzi~\cite{cozzi2013cesium} is a JavaScript library which enables globe rendering in the web browser.
A use case is NASA's Mars Trek~\cite{marstrek} which enables the user to toggle individual map layers and browse the surface of Mars in high detail.
Google Earth, with the Mars feature, also enables layering of many of the datasets currently available from the ongoing missions at the planet as well as the vast amount of surface data for Earth enabling education in a wide range of subjects \cite{patterson2007google}. Similarly to NASA Worldwind, which enables tools to analyze and monitor weather patterns and visualize cities and terrain as well as temporal datasets and trajectories \cite{hogan2006nasa}, the software is, however, not aimed towards contextualization by Universe-sized scale differences in a single unified environment.

%\kallecomment{A bit of a hard time motivating our work over NASA Worldwind}
%\plgrem{Does it cover more than earth?}
%\kallecomment{Yes, it covers other planets too. You can jump between them.}

Public outreach through web interfaces has increased drastically in recent years and we expect to see an increasing rate in flexibility by online software packages when reading and presenting scientific data immersively.
Making use of the benefits of having native C++ as the core language, however, makes it possible to integrate supported libraries and software packages to enable hardware support and high performance. This is enabled using packages such as the Simple Graphics Cluster Toolkit (SGCT) \cite{sgct}, the Geospatial Data Abstraction Library (GDAL) \cite{warmerdam2008geospatial}, the SPICE toolkit \cite{acton1996ancillary} as well as the latest features of modern graphics APIs.

%\kallecomment{Is this more explicit and more positive than the previous text? (see out-commented below)}

%The current state of the art in immersive real time visualization still often requires native appication code due to library support and hardware access.
%This is especially true for our purposes due to required software packages used to integrate our work in a fully dynamic application with focus on data access in a unified visualization environment ~\cite{sgct, warmerdam2008geospatial, acton1996ancillary}.

%Google Earth is an example of a highly specialized rendering system, optimized for the vast amount of high resolution terrain, image, and 3D data constantly in upgrade by the teams of engineers at Google. Recent efforts in web integration, realistic rendering, and VR support has lead to a significant outreach of the most complete virtual Earth visualization to date. The software is limited to Earth and Mars and do not integrate a framework for general astrovisualization.

%NASA Worldwind is an open source software that enables layering of multiple map datasets of the Earth together with tools to analyze and monitor weather patterns and visualize cities and terrain. The software is not suited for public presentations in immersive environments but rather for single users to explore some of the open, static, datasets available for Earth.

%Geospatial Data Abstraction Library (GDAL) is an open source software package and C++ library enabling re-projection and warping of map datasets and can act as a layer of abstraction between the rendering software and the many different types of map formats and projections.

\noindent\textbf{Geographic information systems (GIS)} --- GIS relies heavily on the ability to gather, transform, and visualize geospatial data.
Texture maps and height maps; \emph{digital elevation models} (DEMs) or \emph{digital terrain models} (DTMs), are examples of such data and research and development of software solutions to handle different parts of GIS, such as rendering, have lead to several applications in space visualization.

A GIS software should be able to handle a vast amount of projection formats. When it comes to globe rendering, however, there are standards that are more common than others.
Equirectangular geographic projection is by far the most commonly supported format used for unprojection of globes, it also plays a major role in applications outside of computer graphics, such as the \emph{Global Positioning System} (GPS) \cite{cozzi20113d}.

Geographic projections suffers from well known issues near the poles such as oversampling and polar pinching~\cite{cozzi20113d}. The number of data products of this format, however, makes it useful for our purposes.

%\plgrem{It seems to me a conclusion is missing here, I'm left to wonder what is relevant and not relevant to our case.}

\noindent\textbf{Level-of-detail globe rendering} ---
LOD and multiresolution, out-of-core rendering methods are crucial when visualizing and navigating across large datasets~\cite{luebke2003level}.
By using lower resolution representations to render distant objects, more resources can be directed towards rendering salient objects.
Cozzi and Ring~\cite{cozzi20113d} provide a comprehensive introduction to common methods used for globe rendering.
Examples of earlier LOD techniques for globe rendering include the \emph{real-time optimally adapting meshes}~(ROAM), introduced by Duchaineau~\etal~\cite{duchaineau1997roaming} with improvements made by Hwa~\etal~\cite{hwa2005real}.
Cignoni \etal~\cite{cignoni2003planet} introduced P-BDAM as a planetary scale rendering solution requiring preprocessing and geometry generation.
Although directed towards planetary visualization specifically, there were other methods such as chunked LOD~\cite{ulrich2002rendering} and geometry clip-maps~\cite{losasso2004geometry} that better adapted to the evolution of graphics hardware, the ability to combine multiple data sources, and to generate geometry on the fly.

Kooima~\etal\cite{kooima2009planetary} use a recursive subdivision of an icosahedron to construct the globe using a GPU oriented approach for vertex tessellation and texture atlases for reuse of tiles. Though the rendering algorithm nicely solves polar issues and exploit excellent triangle uniformity, restricted texture sampling precision is a limitation due to the calculation of texture coordinates being performed as an inverse projection using the global transform to the space of the projected map which is either geographic or polar stereographic. Since texture sampling is performed in the fragment shader with 32 bit floating point arithmetic, the resulting texture coordinates are unable to precisely represent imagery of details smaller than 2.39 meters for an Earth sized globe \cite{kooima2009planetary}.
Spherical clip-maps, as presented by Claesen \etal \cite{clasen2006terrain}, also do not align vertex grids with the textures which makes these methods better optimized for globes with smaller scale differences than we require.
%Other methods such as spherical clip-maps \cite{clasen2006terrain} which are optimized for smaller scale differences .

Erikson \etal~\cite{Erikson:2001:HFD:364338.364376} describe a hierarchical LOD (HLOD) method generalized for three-dimensional geometries. For 2D geometries, such as chunks on a globe, each leaf node in a quad-tree corresponds to a part of the complete globe at a specific detail level.
The quad-tree is dynamically updating its nodes by procedures known as \emph{splitting} and \emph{merging} so that high LOD is used only where it is needed.

Ulrich~\cite{ulrich2002rendering} proposed a chunked LOD technique where geometry generation of each chunk requires preprocessing in which the generation process is separated from the run-time selection of detail levels. Cozzi and Ring \cite{cozzi20113d} define sub-processes of chunked LOD rendering as \emph{selection} and \emph{switching} which we adhere to. By defining the chunk geometry in the same hierarchical space as the texture tiles, texture sampling precision errors can be avoided~\cite{cozzi20113d}, see Section \ref{sec:chunkrendering}.

%Early contributions by R{\"o}ttger \etal~\cite{rottger1998real} showed height field rendering using LOD methods.

We adopted the chunked LOD method due to its GPU friendly nature and implicit handling of polar issues that is common for geographically mapped geometry clip-maps methods. In contrast to Ulrich, we generate geometry dynamically on the GPU using vertex texture sampling, similar to Kooima~\etal\cite{kooima2009planetary}. This enables dynamic loading of terrains as well as image-based level blending for textures and height maps.

%To enable dynamic loading of height maps we use a light weight implementation of the chunked LOD method where geometries are generated on the fly. See Section \ref{sec:chunktree}.

Important optimization techniques for HLOD culling are described by Cozzi~\cite{cozzi2008visibility}. Horizon and view frustum culling are important procedures for our purposes to vastly reduce the number of chunks that need to be rendered without compromising image quality.

%%%% MISSING CITATIONS on Terrain Rendering (planetary scale)
%Planet-sized batched dynamic adaptive meshes (P-BDAM\cite{cignoni2003planet}. -- missing
%Survey of semi-regular multiresolution models for interactive terrain rendering \cite{pajarola2007survey}. -- missing
%GPU-friendly high-quality terrain rendering\cite{schneider2006gpu}. 
%Seamless patches for GPU-based terrain rendering \cite{livny2009seamless}. -- missing!
%Planetary-scale terrain composition\cite{kooima2009planetary}. -- missing!

%%%% INCLUDED CITATIONS on Terrain Rendering (planetary scale)
%BDAMâBatched Dynamic Adaptive Meshes for high performance terrain visualization  \cite{cignoni2003bdam}. -- OK!
% Geometry clipmaps: terrain rendering using nested regular grids\cite{losasso2004geometry}. -- OK!
%Real-time optimal adaptation for planetary geometry and texture: 4-8 tile hierarchies\cite{hwa2005real}. -- OK!
%Real-time generation of continuous levels of detail for height fields\cite{rottger1998real}. -- OK!

%Height map datasets can be generated using measured data from altimeters on the satellites, corrected to match an offset from a reference ellipsoid in the direction of the geodetic normal for every point on the surface covered by the dataset.
%Height maps can also be generated using stereoscopic reconstruction.

\noindent\textbf{Stereoscopic reconstruction} -- Although it is only tangentially related to the system that we are proposing, it is valuable to place the many ongoing efforts in the reconstruction of geometry from multiple image sources into context. Koch \cite{Koch95} provided a framework to analyze a scene using dense depth maps and constructing triangulated surfaces from surface points. This work spawned a large body of interesting research that are outside the scope of this manuscript. Wolf \etal~\cite{wolf2000elements} provide a useful overview over the field of using photogrammetry and stereoscopic reconstruction for terrains.

%\begin{enumerate}
%\item the book 3D Engine Design for Virtual Globes
%\item terrain renderer
%\item 3d reconstruction from images (stereoscopic and structure-from-motion)
%\item GDAL
%\item ``virtual presence'' systems
%\item What else?
%\end{enumerate}
%Length: About 1 page\\
%Note:  The page limit was increased to 9+2 pages this year (= 9 pages of manuscript, 2 pages of references). So we should make use of this and cite the hell out of everything that's related


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% Application Scenarios %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Scenarios and Acquisition} \label{sec:scenarios}
The available surface data is the underpinning resource upon which our framework is relying. In this section we therefore provide an in-depth presentation of the data behind the three application scenarios we present in this paper.
We begin by describing the acquisition of terrain data with a high spatial resolution but limited temporal resolution for Mars (Section~\ref{sec:scenario:mars}). We then give an overview of the dynamic time-varying data for Earth (Section~\ref{sec:scenario:earth}). The section is concluded with a description of online image acquisition of a non-orbiting spacecraft, exemplified by New Horizons' flyby of Pluto (Section~\ref{sec:scenario:pluto}).
\fig{scenarios} shows examples of these three application scenarios.

%We chose three distinct application scenarios that cover the majority of planetary surface visualization use cases in the solar system.
%In this section we describe the terrain data we visualize with a high spatial resolution but limited temporal resolution on Mars (Section~\ref{sec:scenario:mars}), the visualization of temporal phenomena on Earth (Section~\ref{sec:scenario:earth}), as well as the online image acquisition of a non-orbiting spacecraft, exemplified by New Horizons (Section~\ref{sec:scenario:pluto}) at Pluto.

%For each of the scenarios, we provide information about the data sources and how these provide specific challenges for a terrain rendering system.

\subsection{Mars} \label{sec:scenario:mars}
%From the pictures of the first Mars missions (Mars 2, Mars 3, Mariner 9), no other planet in the solar system has gained as much attention from the different space agencies.
So far 14 orbiters and 13 landers have been sent to our neighboring planet Mars, which is now the most intensively investigated planet beyond the Earth-Moon system.
The most important large scale imaging campaign of Mars is carried out by the \emph{Mars Reconnaissance Orbiter} (MRO), equipped with an array of cameras, spectrometers, and radar used to image the surface at different resolutions in part for scientific discovery, as well as for the use in landing site evaluation~\cite{zurek2007overview}.
The satellite is in a polar orbit which means that, as the planet rotates underneath it, the instruments will eventually be able to perform measurements at every location on the planet.

The \emph{Mars global digital image mosaic}~(MDIM) is a global image dataset with a resolution of up to 240\,meters/pixel~\cite{archinal2003mars}.
The latest version of this image mosaic, MDIM 2.1 is compiled from Mariner 9 and Viking images with improved accuracy as a result of constraints from the \emph{Mars Orbiter Laser Altimeter} (MOLA) data from the \emph{Mars Global Surveyor} (MGS) spacecraft.
The DEM, assembled from MOLA data, provides an offset from the \emph{Areoid}, Mars' reference ellipsoid, to an average vertical precision of 100\,meters with a resolution of about 450\,meters/pixel~\cite{smith2001mars}.
This global height map provides sufficient resolution to serve as a background for other local terrain models.

The \emph{Context Camera}~(CTX) on the MRO provides gray-scale images of the planetary surface with about 6\,meters/pixel resolution used for providing contextual information for the other instruments observing the Martian surface~\cite{zurek2007overview}.
NASA Ames Research Center assembled a 5\,TB global mosaic of CTX data with roughly 70\% surface coverage.
The CTX camera has mapped over 99\% of the Martian surface after the MRO finished its 50,000th orbit in March 2017 \cite{mroorbit}.

%\kallecomment{The reference to the 50,000 orbits is an online news article. Is it legit? And what info needs to be in the reference?} \plgrem{Yes, you can reference online material, such as a NASA website}

The \emph{HiRISE} camera on the MRO provides images with a spatial resolution of 25\,cm/pixel that have been used to select and evaluate potential landing sites~\cite{mcewen2007mars}.
Both the CTX and HiRISE images can be used for stereoscopic reconstruction of DTM's that are available as local areas of interest on the surface (see Section~\ref{sec:processing}).
McEwen \etal~\cite{mcewen2016people} describes the HiRISE data available online and how the public can contribute in the choice of future target locations.

The main challenges in the Mars browsing application scenario arise from handling data products at various resolutions in a single context.
This requires layer handling and the ability to seamlessly, precisely, and accurately render map data out-of-core with varying levels of detail.
% The accuracy constraints calls for an ellipsoidal globe model where high precision rendering is needed to enable the rendering of high resolution local map and terrain data together with global datasets.

\subsection{Earth} \label{sec:scenario:earth}
%A vast number of data sources of varying spatial and temporal resolutions are available for Earth.
Although Earth's thick atmosphere hinders the acquisition of detailed surface features, the relative ease of placing a satellite into orbit has resulted in hundreds of Earth observing satellites.
Furthermore, a major design constraint for interplanetary spacecraft is the available bandwidth through the \emph{Deep Space Network} (DSN), limiting the amount of data that can be recovered.
This design constraint does not apply for Earth orbiting satellites and thus enables the creation of high spatial and temporal resolution imaging; in many cases these are global images which are updated daily, but more local and higher resolution satellites exist, such as the geostationary GOES-16 which captures an image of the entire western hemisphere every 15 minutes at $10,000^2$ pixels.

We focused on three sun-synchronous satellites, described below, returning daily images of Earth, each covering the majority of Earth's surface area.
The satellites perform scans in a large wavelength spectrum, covering both visible light as well as infrared light ($\approx$~400\,nm~--~14\,\textmu m) and thus provide ample ground for public communication of studies of the surface, oceans, biosphere, and atmosphere.
These datasets are available through a public interface provided by the NASA \emph{Global Imagery Browse Services}~(GIBS) that provides a unified interface to make NASA and \emph{National Oceanic and Atmospheric Administration} (NOAA) imagery available to the general public~\cite{cechini2013expanding}.

The \emph{Moderate Resolution Imaging Spectroradiometer}~(MODIS) instrument has been used on the \emph{Terra} and \emph{Aqua} satellites since 1999 and 2000 respectively.
Its resolution varies between 250\,meters/pixel and 1\,km/pixel~\cite{salomonson1989modis, justice2002overview} and a global image has been generated daily since 2000.
The data shows land boundaries, cloud movements, the study of phytoplankton, and global temperature studies, providing a unique view into the dynamic nature of our planet over this large timespan.
% Due to a limited field of view, the instruments do not have full coverage of the planet at the equator, leading to striped areas of missing data (see Figure?).

The \emph{Visible Infrared Imaging Radiometer Suite}~(VIIRS) instrument on the Suomi NPP satellite provides similar data to the MODIS instrument and has a spatial resolution between 375 and 750 \,meters/pixel since 2012~\cite{schueler2002npoess}.
In addition, it has a pan-chromatic Day-Night-Band that is used to capture the night side of the Earth, thus creating the ability to study the development of cities through their light emission.
% compared to the MODIS instrument has a wider field of view, allowing a continuous coverage of the Earth at all latitudes without missing data.

Another dataset is provided by the Center for Atmosphere Ocean Science at New York University and consists of radar measurements of glacial movements on Greenland. These measurements provide the velocity and height of glaciers as they calve and move into the Atlantic. This results in a time-varying DEM with a temporal resolution of 15 minutes.
% The main challenge in handling time varying map data is the ability to cache a buffer of data read asynchronously.

\subsection{Pluto} \label{sec:scenario:pluto}
The images of the previous two scenarios were acquired by orbiting satellites providing the ability to produce images for an extended period of time.
% In the previous two scenarios the global images were acquired by orbiting satellites that produce images for an extended period of time.
As these orbits are designed to have a low eccentricity, the instruments' surface resolution is constant throughout the orbit, which is very useful when reprojecting images to be used in global maps.
However, not all spacecraft-based image acquisitions are performed from close to circular orbits.
One recent example of a non-orbiting mapping campaign was performed in July 2015 by the New Horizons spacecraft as it was flying past the dwarf planet Pluto.
Prior to the flyby, the best images of Pluto were available from Hubble with a resolution of about 161\,km/pixel.
The main image acquisition instrument on board New Horizons is the \emph{Long Range Reconnaissance Imager}~(LORRI), a gray-scale telescope.
The \emph{Ralph} instrument's primary data source is a Multispectral Visible Imaging Camera~(MVIC) which enables the recovery of spectral information and thus provides color information for the LORRI images.
The \emph{Radio Science Experiment}~(REX) is part of the radio communications system and has the ability to detect radio emissions either directly from Pluto's surface, thus measuring its surface temperature, or Earth-based radio signals that pass through Pluto's atmosphere and thus measure its composition.
Between January and July, the surface resolution of the LORRI instrument gradually increased, reaching about 1\,km/pixel on the day of the flyby and peaking at 50\,meters/pixel during the time of closest approach.

The time varying resolutions provide an opportunity to show the incremental image acquisition, rather than using a static planetary map of varying resolution. Navigation and image data made available by the New Horizons team makes it possible to show the spacecraft's data acquisition process in the context of Pluto and its moons.

%We chose New Horizons as the third scenario due to this challenging varying resolution between image campaigns which is not normally present in other surface mapping projects.
%Furthermore, the varying resolutions provide a opportunity to show the incremental image acquisition to the general public, rather than using a static planetary map of varying resolution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% System Overview %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Globe Browsing System} \label{sec:overview}
In this section we describe aspects of the overall processing pipeline (see \fig{procpipe}) and the technical details required for the three representative application scenarios described in the previous section.
As there are two distinct pathways into the rendering component, some of the scenarios utilize only parts of the described pipeline. More specifically, real-time projection does not utilize tiled textures.

The following sections elaborate on the individual components of our system that provide access to a large variety of imagery sources.
First, the different ways of \emph{Data Access}~(Section~\ref{sec:dataaccess}) in our system is described.
Second, we describe the the \emph{Data Preparation}~(Section~\ref{sec:processing}). If the images are not available in the correct georeferenced projection format or when detailed digital terrain models have to be generated, this is required as a preprocessing step.
Third, we describe the use of \emph{Real-Time Image Projection}~(Section~\ref{sec:imageprojection}) used if navigational data and image times are available for specific mapping campaigns.
%Third, if the scenario requires an iterative image projection and navigational data is available for specific mapping campaigns, \emph{Real-Time Image Projection} is performed in the system as described in Section~\ref{sec:imageprojection}.
Lastly, in Section~\ref{sec:renderingsystem}, we describe the \emph{Rendering System} used for our chunked LOD globe representation, utilizing the common projection format to result in the final three-dimensional globe.

In order to read as many data sources as possible without the need for additional reprojection steps, we chose to use a standardized common georeferenced map projection format; namely the equirectangular geographic projection~\cite{snyder1997flattening}.
From this georeferenced coordinate system, the renderer transforms vertices to achieve the correct mapping of the globe represented by a triaxial ellipsoid.
This makes use of an inverse projection $(x,y,z)^T = P^{-1}(\phi, \theta)$ that maps each geodetic coordinate defined by a latitude ($\phi$), and a longitude ($\theta$) to a Cartesian coordinate on the surface of the reference ellipsoid in the model space of the celestial body as described by Cozzi and Ring \cite{cozzi20113d}.
%For Earth, as an example, this is a transformation from the geographic projection on the Geoid to the commonly used \emph{World Geodetic System 1984} (WGS84) coordinate system~\cite{decker1986world}.

\subsection{Data Access} \label{sec:dataaccess}
All of the data that can be accessed using our system has to be presented as image data.
While this might place a constraint on the data types that can be visualized, it is in most cases possible to represent other interesting information as gray-scale images instead, for example as DTMs, where the pixel value corresponds to a height offset from a reference ellipsoid in the direction of the geodetic normal.

The most common access pattern for global image sources is a web-based LOD service called the \emph{Web Map Service}~(WMS)~\cite{open2006opengis, maso2010opengis}.
While there are different dialects of this standard, it is possible to query a web server for an image covering a specific portion of a celestial body. Image tiles can either be abstracted away or be part of the access depending on the exact implementation of WMS.
The access to these online datasets in our system is unified through the use of the \emph{Geospatial Data Abstraction Library}~(GDAL)~\cite{warmerdam2008geospatial}, which enables access to a variety of WMS dialects through a single interface, thus drastically increasing the number of datasets that are accessible.

The GDAL interface also supports a vast amount of geospatial image data formats including common ones such as IMG, GeoTiff, JPEG2000, and Cube files along with standard image formats such as JPEG and PNG. This is useful to enable access to geographically regional datasets like the HiRISE patches made available from NASA~\cite{mcewen2016people}.

In the case of iterative image projections, such as in the New Horizons scenario, the individual images can be loaded as they are produced and released by the science team.
As these images are not georeferenced, real-time information about the spacecraft and its instruments are required in order to project these images, as described in Section~\ref{sec:imageprojection}.

\subsection{Data Preparation} \label{sec:processing}
For the case of global image datasets, a variety of geographic projection formats are available.
In order to support the layering of multiple data sources for a multimodal surface visualization, we chose to employ the most widely used format, the equirectangular geographic projection, as a single unified projection format for the globes in our system.
If a global image dataset is not provided in equirectanglar projection, we convert the dataset using GDAL.

The HiRISE patches from NASA are referenced in meters and not degrees as required by our equirectangular latitude-longitude rendering pipeline.
Similarly, the exemplified time varying DEMs from the Center of Atmosphere Ocean Science are provided in EPSG:6326 datum which is used by the web mercator projection.
These are examples of datasets that need to be reprojected.
Through GDAL's notion of \emph{virtual datasets}, local patches are treated as sparsely populated global maps. Virtual datasets are defined using XML specifications where properties such as data type, geographic extent, scale factors, offsets, and multiple raster bands can be specified for versatility.

To produce either local or global DTMs, some spacecraft use altimeters that are capable of measuring the height information directly.
If they do not possess instrumentation that produces direct measurements of height information, DTMs can still often be reconstructed using stereoscopic pairs of images in stereophotogrammetry.
The use of the NASA \emph{Ames Stereo Pipeline} (ASP) stereogrammetry software~\cite{moratto2010ames} for content creation becomes a valuable tool in our pipeline for building DTMs from stereo pairs.
ASP works in tandem with planetary mapping methods available through the \emph{United States Geological Survey's}~(USGS), \emph{Integrated Software for Imagers and Spectrometers} (ISIS) ~\cite{gaddis1997overview}.
CTX patches have successfully been generated from stereo pairs to match the MOLA base map of Mars~\cite{broxton2008ames, mayer2016integrated}. We exemplify this using a collection of CTX images covering West Candor Chasma in Valles Marineris on Mars.
Furthermore, it has been shown to work on reconstructing high resolution DTM's using image pairs from the HiRISE camera resulting in a resolution of about 1\,meter/pixel~\cite{li2011rigorous}.

\subsection{Real-Time Image Projection} \label{sec:imageprojection}
In order to support the direct iterative visualization of spacecraft image acquisition, our system interfaces with various tools used in space mission planning and monitoring.
The information these tools provide is used to reconstruct the location and orientation of a spacecraft, together with specifications about the instruments camera system, such as field-of-view and detector sizes.
Utilizing this data, together with the timing at which a specific image was taken, we project images and store them in an equirectangular global image map of the entire globe. This map can then be used as a data source providing image data to the renderer.

The SPICE toolkit provided by NASA's \emph{Navigation and Ancillary Information Facility} (NAIF) is used to load location and orientation data for planets, spacecraft, and instruments~\cite{acton1996ancillary}.
%From this data, we acquire accurate positions and orientations of celestial bodies as well as spacecraft and their instruments throughout the time of a space mission.
Image acquisition and exposure times are either directly provided by the mission scientists or available through NASA's \emph{Planetary Data System} (PDS), which curates information about all past and current space missions~\cite{mcmahon1996overview}.

In order to map image data onto the equirectangular format, we combine position and navigation data from SPICE with image exposure timestamps from PDS. For individual instruments on the spacecraft, virtual camera frustums are constructed. For each image taken by an instrument, latitude-longitude values of the planetary surface are mapped to image coordinates.
The images are projected to the globe using projective texturing as described by Everitt~\etal~\cite{Everitt:2001tg}.
Using this method, the globe's triaxial ellipsoid is rendered from the view point of the instrument. Then the image is applied to the projected globe with the correct geometric distortions. Thus the resulting images can be converted into an equirectangular global mosaic during the visualization of the acquisition.

\subsection{Rendering System} \label{sec:renderingsystem}
The rendering system is primarily driven by the rendering of chunks, which are organized into a quad-tree. Each chunk covers a particular geographic area of the planetary surface. The processing pipeline is outlined in \fig{chunkproc}. First, the chunk tree is processed to reflect the best fit for the given camera view. In this process the chunk tree is evaluated and chunks are either culled, split into child nodes, or merged. The resulting leaves are flagged for rendering, as detailed below in Section~\ref{sec:chunktree}. Second, the system requests tile data from tile providers and will continue to render chunks with the currently available tiles, which we explain in Section~\ref{sec:tilemgmt}. Finally, tiles and layers are combined in the fragment pipeline, presented in Section~\ref{sec:chunkrendering}.
Each chunk is rendered as a skirted regular grid mesh associated with a geodetic area and level of detail, see \fig{chunks-and-tiles}. The grid size is constant per chunk and the height map layers will affect the altitude of the grid vertices.

Separating the representation used for the chunks (the geometry) from the individual tiles (the textures) enables dynamic loading as the geometry LOD can be updated independently from the image information and does not rely on the tile availability.
This is important when multiple layers are combined as each may have its unique resolution.
\fig{chunks-and-tiles} shows this exemplified on a single chunk that is rendered using three layers, each organized in a separate quad-tree.

%Each layer originates from a separate dataset and can be enabled or disabled to show how individual instruments have contributed to our knowledge about a celestial body.
% Each layer is assigned to a specific layer group that defines how the layers are blended in the fragment pipeline.
% Examples of layer groups are: height layers, color layers, color overlays, gray-scale layers, gray-scale overlays, night layers and water masks.
% 
%The chunked LOD globe takes care of updating the chunk tree and evaluating the levels of its nodes.
%It also performs chunk culling as well as rendering of individual chunks.
% 
% In our system, a chunk $C\{x, y\}_k$ is a .
% 
%associated with a set of tiles $T_L\{x, y\}_k$ from all active layers L. \emilcomment{examples instead?}
% 
% 
%\plgrem{The Chunked LOD quad-tree needs a better introduction, but should be outlined in related work. It seems Chunk tree is intended to be detailed in 6.2, so either we simply provide an outline here and do a fwd ref to section Chunk Tree. Rewrite this intro when subsections are written.}
%\kallecomment{Moved introduction of chunked LOD tree to related work. Do you think that description is enough?}
% 
% Using a chunked LOD quad-tree structure, each chunk, unprojected on the reference ellipsoid, can be rendered individually.
% The chunk renderer uses a skirted grid \cite{ulrich2002rendering} which can be mapped on to a geodetic region and rendered in place using one or several layers for texturing or height mapping.
% A geographic tessellation defines the unprojection mapping of the chunks.
% 
% A given tile of index $T_L\{x,y\}_{level}$ does not belong to a specific chunk of index $C_L\{x,y\}_{level}$. Instead, one chunk can be rendered using many tiles, even from the same layer as illustrated in Figure \ref{fig:chunks-and-tiles}. This is useful when performing level blending. See section \ref{sec:fragment_blending}. Examples of tile indices used for different layer types is illustrated in Figure \ref{fig:chunks-and-tiles}.

\subsubsection{Chunk Processing} \label{sec:chunktree}
%The quad-tree containing the chunk information is lightweight as 
\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{figures/rendering-pipeline.pdf}
  \caption{Chunk tree processing and rendering pipeline overview. For each frame, the tree is evaluated and culled before rendering. Tiles are loaded asynchronously to avoid stalling the rendering pipeline.}  \vspace{-4mm}
%  \caption{Chunk tree processing and rendering pipeline overview.}
  \label{fig:chunkproc}%\vspace{-4mm}
\end{figure}


% The \emph{Chunk Tree} contains level-of-detail structure of chunk information. 
%The quad-tree storing the chunk information 
%Since we generally do not perform any preprocessing to generate geometries, height mapping is performed on the fly.

Chunk nodes are organized in a quad-tree and defined as $C\{x, y\}_k$ for a geographic coordinate index $\left( x,y \right)$ and detail level $k$. When $k=1$, the globe is represented with two chunks, one for the Western and one for the Eastern hemisphere.
Chunk tree evaluation involves selection of the desired LOD, limiting the level, and culling of chunks, as illustrated in \fig{chunkproc}.

%Our chunk tree structure is light weight as it does not require vertex data to be stored for every chunk node. We adhere to the common techniques used for chunked LOD rendering but tailor it to our light weight implementation.

\noindent\textbf{Chunk selection} ---
Like Cozzi and Ring~\cite{cozzi20113d}, we select chunks using a unique error metric. Our error metric however is calculated on the fly using the logarithm of the inverse distance from its closest point to the camera position or by an approximation of the chunk's projected area on a sphere around the camera.
The spherical projection is important for multi-display systems where there should be no difference in the error metric of a specific chunk rendered across neighboring screens.

\noindent\textbf{Level limitation} ---
In case a specific tile dataset does not contain any higher resolution tiles the maximum level of the chunk tree is limited.
Since tile datasets can be sparse there is no way to know the highest detail level before the actual reading takes place.
To avoid unnecessary splits of chunk nodes, they should not split unless there is higher resolution data available, that is, if a tile reading operation fails.

\noindent\textbf{Chunk culling} --- To minimize the number of render calls, invisible chunks are culled.
For horizon culling we use the maximum height of the chunk as bounding sphere.
Chunks that are outside of the camera frustum are culled by testing a view space defined axis aligned bounding box of the chunk against the camera frustum.
To perform correct culling, the height layer tiles do need some preprocessing before they can be used for rendering. More specifically, the minimum and maximum values of the tiles are used to specify a minimal bounding hexahedron for each chunk. These values are calculated out of core.
The remaining chunk nodes are flagged as visible for rendering.

\subsubsection{Tile Management} \label{sec:tilemgmt}
A chunk is affected by multiple tiles that have a texture representing a geodetic area. Tiles are uniquely defined using the notation $T_L\{x,y\}_k$, with the layer identifier $L$, the geographic tile index $\left( x,y \right)$ and the detail level $k$.
Tiles are organized in a quad-tree structure so that $T_L\{x,y\}_k$ covers the same area as the four child tiles $T_L\{2x,2y\}_{k+1}$, $T_L\{2x,2y+1\}_{k+1}$, $T_L\{2x+1,2y\}_{k+1}$ and $T_L\{2x+1,2y+1\}_{k+1}$.
\emph{Layers} provide different semantic meaning for groups of tiles. Such usage are for example color layers, color overlays, water masks, or height layers.
The layers' usage only impact the final rendering steps which means that all textures of the same resolution can be cached together and be reused for more recently requested tiles.
%Layers of the same semantic meaning form layer groups and thus provide a way to handle data from different sources the same way if they have the same type of content.

%Tiles are globally arranged into layers, based on their type and source of the tile data. Layers of equal type, such as color images, height maps, and gray-scale images, are organized into layer groups.
%In the rendering process different blending and compositing operations are supported between the layer groups, between the layers in the group, and between the levels within a layer.
Each layer has a corresponding tile provider that the renderer will query using the tile index, representing the corresponding chunk to render.
Tiles are requested for all active layers from the different layer groups.
The ellipsoidal partitioning of the hierarchical tiles are aligned with the chunk partitioning, mapping the tile in a 1:1 relation except for a local offset and scaling. This enables texture sampling to be performed in the local space of each chunk, avoiding precision issues in texture coordinate calculation. Pixel padding, shown as borders around tiles in \fig{chunks-and-tiles}, is also used to achieve seamless blending between tiles.
As a tile provider receives requests for tiles from the rendering thread it will return a tile if present in the in-memory cache, otherwise an empty tile is provided and an asynchronous read request is put on a queue for the provider's tile reading thread.
The tile reader thread uses GDAL to read tile data from a source, GDAL is also caching data locally on disk and maintains an internal cache for recombining and remapping raw data tiles to cover the caller's requested geographic area with pixels.
%The raw tile data reader takes as input a tile index and outputs a raw tile that covers the given geographic area with pixels.
Once the image data is ready, a tile is created and uploaded to GPU memory as a texture.
The tile provider uses an in-memory \emph{least recently used} (LRU) cache so that it can return a cached tile upon request.
%If the tile is already in the cache, the tile provider simply needs to return it and update the cache upon request.
The LRU cache is shared by tile providers of the same layer group and keeps track of the total amount of tile data cached in system memory and on the GPU. When the cache is full, the tile texture of the least recently used tile will be re-used to create a new tile.
%Local hard disk caching is handled by GDAL.
Any other caching is performed outside of the rendering software.
% The use of a local proxy server is helpful when reading from remote servers between sessions due to its ability to cache recently requested data packages.

%\begin{figure}[h]
%  \centering
%    \includegraphics[width=0.5\textwidth]{figures/tile_pipeline.png}
%  \caption{Tile pipeline.}
%\end{figure}

%\kallecomment{Figure}

\subsubsection{Chunk Rendering} \label{sec:chunkrendering}
\begin{figure}
  \centering
    \includegraphics[width=1.0\linewidth]{figures/chunks-and-tiles4.pdf}
  \caption{Rendering chunk $\textrm{C}\left\{ 6,4 \right\}_5$ by combining the information from 9 tiles. For each of the three layer groups $\textrm{T}_\textrm{c}$~(color), $\textrm{T}_\textrm{g}$~(overlay), and $\textrm{T}_\textrm{h}$~(height), the rendering of the main tile $\textrm{T}_\textrm{c}\left\{ 3,2 \right\}_3$,  $\textrm{T}_\textrm{g}\left\{ 6,4 \right\}_4$,  $\textrm{T}_\textrm{h}\left\{ 6,4 \right\}_4$ requires additional levels if level blending is enabled in order to provide interpolation between levels to avoid artifacts.}\vspace*{-4mm}
  \label{fig:chunks-and-tiles}
\end{figure}


Given a valid chunk tree, each chunk that is flagged as visible is rendered individually.
The geometry of the chunks is defined as a 2D regular skirted grid of constant size as seen in \fig{chunks-and-tiles}. We avoid triangle decimation so that the same grid can be used for all chunks. This makes our chunk tree representation light weight and dynamic as vertex offsetting is performed in the vertex shader.

%The downside of using a light weight chunk representation is that we are limited to the use of height mapping for vertex positioning which is done on the fly in the vertex shader.
%This, however saves us from preprocessing.
%This is useful since we can load height layers on demand and adjust each height layer's scaling factor if desired.
Each time a chunk needs to be rendered, all active layers will be queried for the most appropriate tiles.
%A tile can be transformed with a scale and an offset if it does not map 1:1 on to a chunk as seen in \fig{chunks-and-tiles}.
If the requested resolution level is not available, lower levels will be used.
The renderer keeps the two immediately lower resolution levels available as well so that appropriate level blending can be applied, see \fig{chunks-and-tiles}.
We employ two types of rendering techniques for vertex positionig to account for both accuracy and precision.

%This is useful, not only for level blending (Section \ref{sec:fragment_blending}), but if a tile of a certain level is not available.
%the second best thing to do is to go down a detail level and fetch the available tiles of lower detail, adjusting the mapping on the chunk so that the corresponding geodetic area of the tile gets mapped on higher level chunks.


%\alexcomment{Kalle:  The vertices of a chunk need to be defined;  how is a chunk specified?  It was said earlier that the tree is lightweight}\plgrem{A chunk is light-weight as it does not hold any explicit geometry, it simply represents a region in the ellipsoid reference space. Geometry is generated on the flyby a tessellation shader using the height-map layer group. What is still unclear to me is how the LOD is determined for height-map and tessellation. The resolution is perhaps also flexible, or? I assume color and gray-scale layers need higher resolution than height-maps for a good visual appearance.}


%\vspace*{-1.5mm}

\noindent\textbf{Model-space rendering} ---
Using the inverse projection $P^{-1}(\phi,\theta)$, vertices of a given chunk are unprojected from the georeferenced projection to the Cartesian model space of the globe.
This operation is performed in single precision on the GPU and results in the chunk being accurately mapped to the curvature of the reference ellipsoid.
Using this rendering method, the theoretical upper limit of precision will, for a Mars sized globe with radius $r = 3,390 ~km$, be $p = \epsilon r \approx 0.2 ~m$ where $\epsilon = 2^{-24}$ is the machine epsilon of a single precision IEEE floating point number \cite{Axelsson2017Dynamic}. Assuming a screen of width 1,920 pixels, the maximum horizontal length of a tile that covers half of the screen $w = 1,920 / 2$, and does not allow a one pixel error offset, is $l_{max} = p w \approx 194~m$. Approximating the horizontal chunk length as $l = 2 \pi r / 2^{k}$, we get that the chunk level should not exceed $k_{max} = 16$ for model space rendering.

\noindent\textbf{Camera-space rendering} ---
To avoid having an upper bound on the chunk level so that the camera can move arbitrarily close to the surface without presicion errors we use camera space rendering.
The inverse projection $P^{-1}(\phi,\theta)$ can be performed in double precision on the CPU for the four corner points of the chunk which are then transformed to camera space in double precision and uploaded to the GPU in single precision.
The rest of the vertices representing the chunk are then obtained through bilinear interpolation.
This method results in high precision rendering as the origin is practically moved to the center of the camera.
The accuracy, however, is lower since the chunks are approximated as flat surfaces.
We can calculate that the vertical error due the tiles being flat is $e_c = r - r \cos{(2\pi/2^{k + 1})}$.
With the constraint that the screen space error remains smaller than one pixel, i.e. $w e_c / l < 1$, we obtain the minimum allowed chunk level for camera space rendering $k_{min} = 10$.

\noindent\textbf{Combined model- and camera-space rendering} ---
To account for both the accuracy, required for large curved chunks, and precision, required for smaller chunks close to the camera, we use a cutoff level, $\lambda$, to determine whether to use the model space or the camera space rendering as illustrated in \fig{chunkproc}.
Setting  $\lambda$ to a value between $k_{min} = 10$ and $k_{max} = 16$, satisfies both of the error criteria. These calculations are invariant of $r$ which means that the same cutoff level can be used for globes of any size.

\iffalse

\noindent\textbf{Model-space rendering} ---
Using the inverse projection $P^{-1}(\phi,\theta)$, vertices of a given chunk are unprojected from the georeferenced projection to the Cartesian model space of the globe.
This unprojection is performed in single precision on the GPU and results in the chunk being accurately mapped to the curvature of the reference ellipsoid.

%\vspace*{-1.5mm}

\noindent\textbf{Camera-space rendering} ---
The same inverse projection can be performed in double precision on the CPU for the four corner points of the chunk which are then transformed to camera space in double precision and uploaded to the GPU in single precision.
The rest of the vertices representing the chunk can be obtained through bilinear interpolation for the chunk and then mapped onto the reference ellipsoid.

This method results in high precision rendering as the origin is practically moved to the center of the camera.
The accuracy, however, is lower since the chunks are approximated as flat surfaces due to the bilinear interpolation.

%\vspace*{-1.5mm}

\noindent\textbf{Combined model- and camera-space rendering} ---
To account for both accuracy, required for large curved chunks, and precision, required for smaller chunks close to the surface, We use a cutoff level, $\lambda$, to determine whether to use the model space or the camera space rendering as illustrated in \fig{chunkproc}.
All smaller chunks, which are better approximated as flat surfaces, are rendered with the camera space method since their vertical error metric, due to curvature, is smaller.
As an example, a cutoff level of $\lambda = 10$ for Mars, where $r=3,390$ km, leads to a maximum vertical error of $e = r - r \cos{(2\pi/(2^{\lambda + 1}))} \approx 16$ meters.
With chunks of horizontal length $l = 2\pi r / 2^{\lambda} \approx 20.8$ km, 16 meters vertical error is not noticeable. We can exemplify this using a screen with a resolution of 1,920 pixels and a chunk covering half the screen as an overestimate. The resulting error is $e / l  \times 1,920 / 2 \approx 0.7$ pixels and decreasing for higher levels.
Similarly, the maximum precision error allowed for this level to avoid one pixel offsets is $20,800 / (1,920 / 2) \approx 22$ meters. The numbers used need to accurately represent $3,390,000 / 22 \approx 154,000$, which is smaller than the highest accurate IEEE floating point number \cite{kooima2009planetary}. Therefore, all lower levels can be rendered with the model space method without precision issues.

\kallecomment{Rewrite the camera / model space rendering to be a derivation of the cutoff level.}

\fi

%\vspace*{-1.5mm}

\noindent\textbf{Fragment blending} ---
To account for versatility and seamless level switching %\cite[p. 450]{cozzi20113d}
we perform three different types of fragment blending.

Layer group blending is performed to blend different layer groups by sampling and compositing each active layer group to get the final color, as shown in \fig{fragpipe}.
Examples are night layers which will be sampled on the night side of the globe, and water layers which are used to add a specular component to the final color.

Layer blending is performed so that many layers in the same layer group can be rendered together. Layers can be manually ordered and combined using per-layer settings such as opacity or blend mode. Furthermore, using alpha blending, local high-resolution patches can be rendered on top of other datasets with lower resolution.

Level blending is performed to smoothly transition between levels and avoid artifacts. It differs from Ulrich's time-varying blending \cite{ulrich2002rendering} in that we calculate an interpolation parameter for each fragment similar to Kooima \etal ~\cite{kooima2009planetary} depending on the camera distance that is used to interpolate between the tile of the current chunk level and the corresponding tiles of the two lower-level tiles, see Figure~\ref{fig:chunks-and-tiles}.

We use shader preprocessing and recompilation to opt out any unused blending option or layer as they are togglable interactively.

\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{figures/fragment-blending.pdf}
  \caption{Layer blending. Gray-scale overlays affect only the value component of the base color in HSV space. Water masks are used to change the specular reflection in the surface shading. Night tiles are useful for Earth where cities illuminate the side that is not facing the Sun. Geographical information can be added using overlay tiles.}
  \label{fig:fragpipe}\vspace{-4mm}
\end{figure}

\iffalse

\begin{figure}
    \centering
    \begin{subfigure}[tb]{0.24\textwidth}
    	\includegraphics[width=\textwidth]{figures/blending1b.pdf}
	\caption{Blending disabled.\\ Only need of sampling one tile.}
    \end{subfigure}
    \begin{subfigure}[tb]{0.24\textwidth}
    	\includegraphics[width=\textwidth]{figures/blending2b.pdf}
	\caption{Blending enabled.\\Two tiles or more are required.}
    \end{subfigure}
    \caption{Blending on a per fragment basis. The level interpolation parameter $t$ is used to calculate level weights $w_1 = 1-t$ and $w_2 = t$. The level weights are used to combine sampling from high level tiles with low level tiles. In practice we can use up to three tiles of different level, in the figure the concept is illustrated using two tiles.}  \vspace{-4mm}
    \label{fig:switchingblending}
\end{figure}

\fi


\subsubsection{Time Varying Datasets}
Due to the abstraction of tile requests, we can define a layer type which can be updated in the rendering loop. For time varying datasets, a time stamp corresponding to the current simulation time is used to instantiate a tile provider for any given time step.
We store a template defining the dataset without a specific time stamp. This template can be a file path or a GDAL WMS configuration that contains a time stamp key along with metadata defining time format, start time, end time, and temporal resolution. When issuing calls for tiles and the corresponding tile provider needs to be created, the time stamp key is replaced with a time stamp and a new dataset is opened.

As an example, a folder containing several image files can be used as a source for a temporal dataset as new files are opened once the simulation time reaches the next valid interval. If the dataset corresponding to the given time interval is already open, the temporal tile provider reads the tile corresponding to the given tile index.
Tiles from temporal tile layers do not differ from other tiles so they can be saved in the same cache and returned upon request.

Opening datasets and streaming tile data upon request can cause delay in tile loading if the time step of the simulation is significant. This can lead to a lot of reading within a short amount of time. Therefore, memory buffering becomes important for temporal datasets. Memory buffering can either be performed by pre-caching tiles when initializing the software or by browsing to the specific time intervals and let the tiles load as they are requested. Once in memory, the switching of tiles depending on time interval is not different from regular tile loading. Therefore, the layers can be animated by scrubbing the global simulation time forward and backward.

\subsubsection{Atmosphere}
Our application use an implementation of atmospheric effects as proposed by Elek~\cite{elek2009rendering}. The teaser image as well as \fig{marsganges} and \fig{earthdynamic} shows how it enables more realistic representations of Earth and Mars.

\subsection{Multi Display Systems Rendering} \label{sec:multidisplaysystems}
The OpenSpace software uses a cluster rendering software package known as the \emph{Simple Graphics Cluster Toolkit} (SGCT)~\cite{sgct} which supports image warping and blending for projection on dome displays and other clustered environments using several rendering machines (see Figure~\ref{fig:power_wall}).
Synchronization across rendering nodes is based on GenLocking at the graphics hardware level and SGCT furthermore provides support for synchronization of selected application data at frame rendering level, ensuring that all nodes in the cluster renders a given frame with the same camera and time state.
Tile loading and chunk LOD selection is autonomous on each node, which visually can result in minor differences in the projector blending regions. However, due to the use of camera direction independent chunk selection algorithms, each rendering node will request the same tiles to be used across projector edges.
% The loading time can differ however since each node uses its own tile reading request queue
% This is generally not an issue as navigation and movement in dome settings need to be slow to moderate to avoid motion sickness.

\begin{figure}[b]\vspace{-4mm}
    \centering
    	\fbox{\includegraphics[width=1.0\linewidth]{figures/power_wall.jpg}}
        \caption{Browsing the surface of Mars in a clustered environment rendering on the power wall at the University of Utah. The system uses eight rendering nodes, each connected to four screens.}
    \label{fig:power_wall}
\end{figure}

\section{Results}
%\anderscomment{We have to make sure that this section doesn't become a repeat of the application scenarios section with some more details added.
%}

Here we present our results exemplified by the three application scenarios introduced in Section \ref{sec:scenarios}. We perform a globe browsing descent towards the surface of Mars measuring frame time and the size of the chunk tree. We also show how the Earth can be rendered with temporally varying textures of high resolution. Finally, we exemplify how our application is used to communicate the data acquisition process of the New Horizons flyby of the Pluto system.

\begin{figure*}
\includegraphics[width=\linewidth]{figures/marsbrowsing5_compressed.pdf}
\caption{Globe browsing on Mars: top graph shows size of the chunk tree along with the total render time. Atmosphere is not rendered. 1) Global CTX Mosaic enabled, 2) Local CTX patch enabled, 3) Local HiRISE patch enabled. A) Camera approaches Mars, B) Valles Marineris, C) West Candor Chasma, D) Layered rock outcrops in South West Candor Chasma, E) Camera tilted up, F) Camera rotated and then tilted down.}  \vspace{-4mm}
\label{fig:globebrowsingmars}
\end{figure*}

\begin{figure}[t!]
  \centering
    \includegraphics[width=1.0\linewidth]{figures/load_times.pdf}
  \caption{Loading times for different data sources illustrated by the number of rendered chunks when enabling the layer as the camera is placed 50\,meters from the surface facing the horizon.}
  \label{fig:load-times}\vspace*{-4mm}
\end{figure}

\subsection{Globe Browsing on Mars}


%We chose Mars as one of the application scenarios due to the availability of high resolution images and can thus demonstrate the ability of the system to handle large scale differences.

In our test session, illustrated in Figure \ref{fig:globebrowsingmars}, Mars is rendered using the global Viking Color mosaic 2.1 as a base color layer.
The global MOLA height layer is used as a base DTM.
Furthermore, in order to achieve more detail, we include the Global CTX mosaic, as well as local patches that contain DTM and textures from the CTX and HiRISE camera.

The computer used for the measurements is an Intel Xeon E5-1620 3.60~GHz with 8~GB RAM and an NVIDIA GeForce GTX-780 GPU.

\fig{globebrowsingmars} shows how the frame time depends on the loading of data and the size of the chunk tree.
When activating new layers, the frame time increases due to the need of shader recompilation.
By descending to the surface, more of the chunk tree is traversed. However, the number of rendered chunks stays constant due to culling.
Tilting the camera leads to new tile reading calls which in turn causes the chunk tree to grow over time as more tiles become available, see \fig{frustum_mars}.
Following point E, the camera is rotated and thus decreasing the chunk tree due to unavailable tile data in areas previously invisible to the camera.
As soon as the data is read and available, the chunk tree grows since it no longer is limited by the unavailable data.
As soon as the camera is tilted down again, the number of rendered chunks decreases.

To further show how the different data products contribute to the chunk tree, \fig{load-times} illustrates the loading time for each of the Mars datasets with the camera positioned above the HiRISE patch.
The final number of rendered chunks is directly dependent on the data source used due to the chunk level being limited by the availability of data and therefore differs between datasets.
This scenario combines data sources stored on local WMS servers (CTX Mosaic and MOLA) and local patches stored on a spinning hard disk (HiRISE) and there is no significant difference between the loading times of the two.

\begin{figure}[t!]
    \centering
    	\fbox{\includegraphics[width=0.98\linewidth]{figures/frustum_mars.jpg}}
        \caption{Illustration of camera frustum and horizon culling. Model space rendering of chunks (green) is performed at higher levels, while camera space rendering (red) is performed at lower levels.}
    \label{fig:frustum_mars}\vspace*{-4mm}
\end{figure}

\begin{figure}[b!]\vspace*{-4mm}
    \centering
    	\fbox{\includegraphics[width=\linewidth]{figures/glacier.jpg}}
        \caption{Rendering of a single timestep of the Helheim glacier's movements in Greenland as acquired by radar measurements. With a cadence of 15 minutes, it becomes possible to observe the calving of the glacier.}
    \label{fig:glacier}
\end{figure}

The Mars globe browsing sessions have been used in a large number of public presentations to explain the history of Mars exploration, and the ability to contextualize events pertaining to the rovers on the surface.
These public education events using the available Mars data have been ongoing for the past year and reached thousands of people using multi display systems and online streaming for public outreach. One such example of an event was a globe browsing session in the Hayden Planetarium seen by 400 people on site and live streamed on YouTube on June 5\textsuperscript{th} 2017 entitled "Mars as never seen before".

\subsection{Temporal Datasets on Earth}

In addition to ongoing public events related to the exploration of high resolution surface imagery on Mars, other public live shows have been held using our system to show dynamic processes on our own planet.
On April 22\textsuperscript{nd} 2017, coinciding with Earth day, an event connecting five locations around the world was held to discuss the impact of global warming on Earth with live audiences.
This event consisted of multiple global NOAA datasets that show, among other parameters, precipitation, temperature, and the aerosol content of the atmosphere.

Moreover, to show how the Sun is directly affecting our planet, coinciding with Sun-Earth day, an event was held on June 27\textsuperscript{th} 2017 at the Hayden Planetarium.
By showing volumetric datasets from NASA's \emph{Community Coordinated Modeling Center} (CCMC), simulating solar activity visualized with time-varying field lines and ray casted volumes, in the same context as measured data in the form of global equirectangular time-varying datasets close to Earth, our software was used to visualize how the solar flares of 2012 related to the number of charged particles that impacted our planet.

%Another dataset is provided by the Center for Atmosphere Ocean Science at New York University and consists of radar measurements of glacial movements on Greenland (see Figure~\ref{fig:helheim-glacier}). These measurements provide the velocity and height of glaciers as they carve and move into the Atlantic. This results in a time-varying DEM that changes every 15 minutes. While displaying this in a 3D environment is beneficial for the domain scientists, further research into time-varying DEMs is necessary.

By rendering time-varying DEMs in 3D, domain scientists working on generating and maintaining such datasets are able to contextualize their work. Further research into time-varying DEMs is necessary but we show already in \fig{glacier} the calving of the Helheim glacier ridge.

By accessing other imagery repositories such as the ones mentioned in Section \ref{sec:scenario:earth} we are able to visualize hundreds of different scientific parameters animated over time. The list includes, but is not limited to, surface reflectance, cloud pressure, acids and oxides, temperatures, snow and ice coverage, soil and vegetation indices, chlorophyll levels, and more. Together with reference layers such as labels, border lines, coast lines, and roads these layers can easier be contextualized.
\fig{earthdynamic} shows corrected reflectance and maximum air temperature, both with a temporal resolution of one day.

By utilizing this data driven approach to animated planetary visualization we can show that our planet is far from static. In fact, it can act as a tool in educating the general public about the impact of human society on everyone's planet.


%In addition to the ongoing public events using the high-resolution Mars data from the previous section, a global, public event is planned on April 22nd coinciding with Earth Day, that will celebrate the knowledge about our planet and make this data available to the general public.
%This event will connect five locations around the world and provide a video stream of Earth scientists utilizing this system to explain the complex temporal behaviors of the Earth and how we achieved these measurements.
%The scientists will be located at the Hayden planetarium in New York, but will be interacting with every audience member around the world.
%Utilizing the presented system, the scientists will be able to place their own data and measurements into their accurate location, and thus context, in order to communicate their data to the public.

%\fig{temporal_earth} (top row) contains images from the time-varying VIIRS datasets over a few days, showing the location and movement of clouds on Earth. The bottom row shows the local fluctuations of the sea surface temperate over a few months. Both datasets are provided by NASA GIBS.
%By combining these data sources it becomes possible to contextualize, for example, the creation of a hurricane in the Atlantic by showing the sudden temperature change that coincide with the generation of a large cloud area.

%As the \emph{Aqua} and \emph{Terra} satellites have been collecting data for the last 17 years, it is further possible to show the long term evolution of individual locations on Earth, such as the drying of Lake Mead.
%Showing these large scale changes at a variety of locations around Earth plays an important role in educating the general public about the impact of human society on everyone's planet.

\iffalse

\begin{figure}[b]
    \centering
    \begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_viirs1.png}
	\end{subfigure}
    \begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_viirs2.png}
	\end{subfigure}
    \begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_viirs3.png}
	\end{subfigure}

	\begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_sea_surface1.png}
	\end{subfigure}
    \begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_sea_surface2.png}
	\end{subfigure}
    \begin{subfigure}[tb]{0.32\linewidth}
    	\includegraphics[width=\textwidth]{earth_temporal/earth_temporal_sea_surface3.png}
	\end{subfigure}
    \caption{These images show the temporal behavior of Earth's cloud formations (top row) from the Suomi NNP satellite's VIIRS instrument over the course of a week. The sea surface temperature (bottom row) is retrieved over the course of three months from the \emph{Group for High-Resolution Sea Surface Temperature} (GHRSST).}  \vspace{-4mm}
    \label{fig:temporal_earth}
\end{figure}

\fi

\subsection{Visualizing Acquisition on Pluto}
NASA's New Horizons mission flew by the Pluto system on July 14th, 2015 and took measurements with its seven instruments.
Of special interest are the LORRI and \emph{Ralph} instruments, that provided images of Pluto and its moons' surfaces, as well as REX, measuring Pluto's atmosphere.
%In our system, the images are projected using the method described in Section~\ref{sec:imageprojection}.
The user can follow the progression of the spacecraft over time and see the instrument activities.
In addition to visualization of the instrument frustums and projected images, the measurement times for all instruments are presented to the user. 

This mission visualization was presented to about 2,500 people during a public, global event with 13 different locations.
During a 2\,h live show, which coincided with New Horizons closest approach to Pluto, experts on the mission team explained details of the desired outcome using OpenSpace as the source of contextualization for this information, see \fig{nh_event}.
In addition to the live audience in the participating locations, a video stream of the event was made available. This was also later provided as a video-on-demand, called ``Breakfast at Pluto''.
The event was described in the poster on public dissemination of space mission profiles by Bock~\etal~\cite{Bock_2015}.
Some initial results from the New Horizons mission are given by Stern~\etal~\cite{stern2015pluto}.
The visualizations have also been used in several post flyby events showing the incremental update of our knowledge about this object to the general public.

\begin{figure}[b!]
\vspace*{-4mm}
    \centering
    	\fbox{\includegraphics[width=\linewidth]{figures/breakfast.jpg}}
        \caption{OpenSpace was used to visualize how New Horizons captured images of Pluto on July 14th, 2015. Sites from all over the world connected in a live event during the flyby. The image shows the New York site with host Neil deGrasse Tyson.}%\vspace*{-3mm}
    \label{fig:nh_event}
\end{figure}

%\alexcomment{Previous New Horizons text:}
%One of the main science instruments that is used on spacecraft are digital cameras that take images. In order to present these images to the audience, they are projected onto the target body using projective texturing as described by Everitt~\etal~\cite{Everitt:2001tg}. As images are mostly acquired in quick succession, this leads to a mosaic covering of the target, as was done on by the New Horizons mission on Jupiter in 2007 (see \fig{jupiter}). In the cases where the whole image does not cover a body, a virtual image plane is added that contains the remainder of the image (see \fig{pluto}).

%NASA's mission will flyby the Pluto system on July 14th, 2015 and will take measurements with its seven instruments. Of special interest are the LORRI and RALPH instruments, that will take images of Pluto's and Charon's surface, as well as REX that will take radio measurements of Pluto's atmosphere (see \fig{radio}). In our system, the images are projected using the method described above and the REX occultation measurements are represented by a line connecting the spacecraft and Earth. The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping, for example the SWAP instrument measures solar wind density values. The components for this mission were shown at the American Museum of Natural History's Evening for Educators event on May 14th that part of the Pluto Palooza event series. In addition, the July 14th flyby will be shown at various locations around the globe.

%As one of the major motivations for developing this method, spacecraft are inherently small objects that need to be visualized precisely with respect to an object, mostly planets, that they are inspecting.
%One example is shown in \fig{teaser} where the New Horizons spacecraft is shown at 80\,Mm distance to Pluto and around $6 \cdot 10^{12}$\,m away from the Sun.
%As described in Section~\ref{sec:floating-point-numbers}, floating point numbers only have a precision of about 300\,km at the distance of Pluto, which would make a traditional approach infeasible. With our method, however, it is possible to show the entire mission with the required precision.


%\anderscomment{This paragraph can go if we need to save space}
Space missions, such as \emph{New Horizons} or \emph{Cassini}, are very expensive endeavors carried out in the public interest. These missions are planned years in advance in order to maximize the scientific output they produce. Due to this streamlining and the lack of available tools for the general public, the spacecraft's actions during the scientific exploration can be difficult to understand and communicate. The New Horizons mission is an example of how our system allows mission scientists to explain their findings in context, and indeed in current time, and also allows them to preview different mission plans, while providing interesting content for the public engagement in science and technology.

%\noindent {\bfseries New Horizons.} NASA's mission will flyby the Pluto system on July 14th, 2015 and will take measurements with its seven instruments. Of special interest are the LORRI and RALPH instruments, that will take images of Pluto's and Charon's surface, as well as REX that will take radio measurements of Pluto's atmosphere (see \fig{radio}). In our system, the images are projected using the method described above and the REX occultation measurements are represented by a line connecting the spacecraft and Earth. The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping, for example the SWAP instrument measures solar wind density values. The components for this mission were shown at the American Museum of Natural History's Evening for Educators event on May 14th that part of the Pluto Palooza event series. In addition, the July 14th flyby will be shown at various locations around the globe.

% \begin{enumerate}
% \item Screenshots
% \item Render times
% \item Loading times
% \end{enumerate}

\section{Conclusion and Future Work} \label{sec:conclusion}
In this paper we have presented an application which enables data-driven visual browsing of planetary surfaces with dynamic resolution in space and time.
The application makes it possible to generate global views as well as virtual terrain fly-overs.
One of the novel aspects of our work is that it contextualizes the surface data in a seamless astrovisualization environment making it possible to travel from one globe to another, or even visit spacecraft at past, current, or future locations.
The platform used is the OpenSpace~\cite{Bock2017} software which builds on a dynamic scene graph and the capability to accurately position and visualize celestial bodies and spacecraft.
Another key component presented is the capability to visualize the data acquisition process from the spacecraft perspective, letting the user appreciate the engineering efforts behind complex space exploration missions whilst appreciating the origin of the data which drives scientific discovery.
With science communication as a primary target the software has already proven to create engaging immersive experiences in large scale dome theaters.
We have developed a tool useful for scientists, who want to present their work in a contextualized manner, and for the general public who wish to explore accurate and open data representation of celestial bodies.

As future work we will further explore the seamless addition of detailed data from surface-based missions such as the Mars rovers. We will also add more data sources such as simulated data of abstract planetary properties.
We also aim to add GIS tools to better aid scientists whereas the current main focus of the application is for public presentations.
Intuitive and intelligent context dependent navigation models for multi-scale data is also an area with interesting challenges. 

\acknowledgments{
We would like to acknowledge the Wallenberg Foundation, the Swedish e-Science Research Center (SeRC), and ELLIIT for their support of this work.
Parts of this work were supported by Vetenskapsr\aa det under award VR-2015-05462, NASA under award No NNX16AB93A, the Moore-Sloan Data Science Environment at New York University, and NSF awards CNS-1229185, CCF-1533564, CNS-1544753, and CNS-1730396.
The authors would like to thank Ryan Boller, Lucian Plesea, and the NASA GIBS team for their help, and Jonathas Costa for the implementation of atmospheric effects. We would also like to thank the anonymous reviewers who provided valuable comments to improve the final quality of the paper.
}

%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{references}

\end{document}

